#IMPORTING LIBRARIES REQUIRED FOR DATASET
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import seaborn as sns

#IMPORTING WARNINGS IGNORES ALL THE ERRORS
import warnings
warnings.filterwarnings('ignore')

 LOADING THE DATASET
  
  df=pd.read_csv("C:\\Users\\lahar\\OneDrive\\Documents\\ml\\IRIS.csv")

  #BASIC CHECKS

df.head(10)
df.columns
df.describe()
df.info()
df.isnull().sum()
df.describe()
df.duplicated()

  EDA
  
  # Pairplot: Visualize relationships between features, colored by species
sns.pairplot(df, hue='species')
plt.show()

for i in df.columns[:-1]:
    sns.scatterplot(data = df, x = i, y = 'species')
    plt.show()

  from itertools import combinations

features = df.columns[:-1]  # All numeric features
for x_feat, y_feat in combinations(features, 2):
    plt.figure(figsize=(6, 4))
    sns.scatterplot(data=df, x=x_feat, y=y_feat, hue='species')
    plt.title(f"Scatterplot: {x_feat} vs {y_feat}")
    plt.legend(title='Species')
    plt.show()

  for i in df.columns[:-1]:
    sns.boxplot(data=df,x=i,y='species')
    plt.show()
    # Plot boxplots of all numerical features grouped by species

  for col in df.columns[:-1]:
    plt.figure(figsize=(6, 4))
    sns.histplot(data=df, x=col, hue='species', kde=True, element='step', stat='density')
    plt.title(f"Histogram of {col} by Species")
    plt.xlabel(col)
    plt.ylabel("Density")
    plt.grid(True)
    plt.show()

  #Correlation heatmap (for numeric features only)
sns.heatmap(df.drop('species', axis=1).corr(), annot=True, )
plt.title("Feature Correlation")
plt.show()

x=df.drop(['species'],axis=1)
y=df['species']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)

#As the target variable is categorial , it is converted to numerical uim=ng labelencoder
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()

# Fit only on y_train
y_train_encoded= le.fit_transform(y_train)

# Then transform y_test using the same encoder
y_test_encoded = le.transform(y_test)

MODEL TRAINING
  
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(C=1.0)
classifier.fit(X_train, y_train_encoded)
y_pred = classifier.predict(X_test)
calculation = pd.DataFrame(np.c_[y_test_encoded,y_pred], columns = ["Original","Predicted"])
calculation
from sklearn.metrics import accuracy_score

accuracy_score = accuracy_score(y_test_encoded,y_pred)
print("Accuracy score of testing data: " + str(accuracy_score))

#Determines the true +ve , true -ve,false +ve,false -ve,
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test_encoded, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)
disp.plot()

#This checks how well your model performs across different splits of data
from sklearn.model_selection import cross_val_score
scores = cross_val_score(classifier, x, le.fit_transform(y), cv=5)
print("Cross-validation average accuracy:", scores.mean())

 #trying different classifiers to compare performance:
del accuracy_score
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

models = {
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier()
}

for name, model in models.items():
    model.fit(X_train, y_train_encoded)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test_encoded,y_pred)
    print(f"{name}: Accuracy = {acc:.2f}")
